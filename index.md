---
layout: default
comments: false
title: Home
permalink: /
---

## Biography

Zhiming is currently an assistant professor at the Department of Computer Science, [Shanghai University of Finance and Economics](http://english.sufe.edu.cn/). He obtained his Ph.D. in Computer Science from [Shanghai Jiao Tong University](http://en.sjtu.edu.cn/) in 2020. He received his B.S. in Computer Science from the [ACM Class](https://acm.sjtu.edu.cn/home) at Shanghai Jiao Tong University in 2014.

Zhiming has a broad interest in fundamental techniques and theories of machine learning, including optimization, generalization, generative models, representation learning, learning theory, etc. Ultimately, he hopes to contribute to laying a solid foundation for applying artificial intelligence in the real world.

## Docs & Links

[CV](https://raw.githubusercontent.com/ZhimingZhou/zhimingzhou.github.io/master/assets/Zhiming_Zhou_Resume.pdf)

[Google Scholar](https://scholar.google.com/citations?user=b8YJ1EMAAAAJ&hl=en) 

[Github](https://github.com/ZhimingZhou)

[Prospective Students](https://zhimingzhou.github.io/Posts/Prospective-Students/)

## Selected Publications 

Residual Multi-Task Learner for Applied Ranking.
  \[[openreview](https://openreview.net/forum?id=dOWWNW9CJ3)\]\[[acm](https://dl.acm.org/doi/abs/10.1145/3637528.3671523)\]\[[slide](https://github.com/ZhimingZhou/zhimingzhou.github.io/raw/ff28a097b745dba9ff366ba0e6f928d2c4c043a9/assets/Residual%20Multi-Task%20Learner%20for%20Applied%20Ranking.pdf)\]
- Cong Fu, Kun Wang, Jiahua Wu, Yizhou Chen, Guangda Huzhang, Yabo Ni, Anxiang Zeng, **Zhiming Zhou**.
- ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2024.

Recurrent Temporal Revision Graph Networks.
  \[[openreview](https://openreview.net/forum?id=B3UDx1rNOy)\]\[[acm](https://dl.acm.org/doi/abs/10.5555/3666122.3669159)\]
  \[[arxiv](https://arxiv.org/abs/2309.12694)\]
- Yizhou Chen, Anxiang Zeng, Guangda Huzhang, Qingtao Yu, Kerui Zhang, Cao Yuanpeng, Kangle Wu, Han Yu, **Zhiming Zhou**.
- Annual Conference on Neural Information Processing Systems (NeurIPS), 2023.

Clustered Embedding Learning for Recommender Systems.
  \[[acm](https://dl.acm.org/doi/abs/10.1145/3543507.3583362)\]
  \[[arxiv](https://arxiv.org/abs/2302.01478)\]
- Yizhou Chen, Guangda Huzhang, Anxiang Zeng, Qingtao Yu, Hui Sun, Heng-yi Li, Jingyi Li, Yabo Ni, Han Yu, **Zhiming Zhou**.
- ACM Web Conference (WWW), 2023.

Lipschitz Generative Adversarial Nets.
  \[[pmlr](https://proceedings.mlr.press/v97/zhou19c.html)]
  \[[arxiv](https://arxiv.org/abs/1902.05687)\]
  \[[slide](https://icml.cc/media/Slides/icml/2019/halla(11-14-00)-11-15-10-4628-lipschitz_gener.pdf)\]
  \[[code](https://github.com/ZhimingZhou/AdaShift-LGANs-MaxGP-refactored)\]
  \[[code](https://github.com/ZhimingZhou/LGANs-for-reproduce)\]
  \[[code](https://github.com/ZhimingZhou/MaxGP-MaxAL-for-reproduce)\]
- **Zhiming Zhou**, Jiadong Liang, Yuxuan Song, Lantao Yu, Hongwei Wang, Weinan Zhang, Yong Yu, Zhihua Zhang.
- International Conference on Machine Learning (ICML), 2019.

AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods. 
  \[[openreview](https://openreview.net/forum?id=HkgTkhRcKQ)\]
  \[[arxiv](https://arxiv.org/abs/1810.00143)\]
  \[[poster](https://s3.amazonaws.com/postersession.ai/bd0f7f0b-ecaa-4164-aeb6-d0cf181cc27b.jpg)\]
  \[[code](https://github.com/ZhimingZhou/AdaShift-LGANs-MaxGP-refactored)\]
- **Zhiming Zhou**\*, Qingru Zhang\*, Guansong Lu, Hongwei Wang, Weinan Zhang, Yong Yu.
- International Conference on Learning Representations (ICLR), 2019. 

Activation Maximization Generative Adversarial Nets.
  \[[openreview](https://openreview.net/forum?id=HyyP33gAZ)\]
  \[[arxiv](https://arxiv.org/abs/1703.02000)\]
  \[[code](https://github.com/ZhimingZhou/AM-GANs-refactored)\]
  \[[code](https://github.com/ZhimingZhou/AM-GANs-for-reproduce)\]
- **Zhiming Zhou**, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, Yong Yu.
- International Conference on Learning Representations (ICLR), 2018.

Sparse-as-Possible SVBRDF Acquisition. 
  \[[acm](https://dl.acm.org/doi/10.1145/2980179.2980247)\]
  \[[pdf](http://yuedong.shading.me/project/sparsesvbrdf/sparsesvbrdf.pdf)\]
  \[[slide](https://drive.google.com/file/d/16gUKZoQH4HiQ61gEQ-YFs6v9WTEOSixf/view?usp=sharing)\]
- **Zhiming Zhou**, Guojun Chen, Yue Dong, David Wipf, Yong Yu, John Snyder, Xin Tong.
- ACM Transactions on Graphics (TOG) - ACM SIGGRAPH Asia, 2016.





<!--
- <details><summary>Click to expand a brief introduction.</summary>We significantly reduce the number of images required for spatially-varying surface reflectance (SVBRDF) acquisition, by solving an exact low-rank representation and chasing an extreme sparsity. The number of images required dropped from thousands to tens, and high-quality SVBRDF acquisition from a single image became possible for the first time.</details>
-->

<!--
- <details><summary>Click to expand a brief introduction.</summary>We study the convergence issue of Adam optimizer. With the proposed concept *net update factor*, we showed that the key issue in Adam lies in its biased adaptive learning rate caused by the correlation between the adaptive term v_t and the current gradient g_t, and a temporal shift operation is proposed to solve such an issue. Our new understanding of the role of v_t also free v_t from its traditional update rule, leading to more interesting variants. Particularly, with dimension reduction operation in v_t, we achieve the so-called adaptive learning rate SGD, which removes the global gradient scale but keeps the relative scales.</details>
-->

<!--
- <details><summary>Click to expand a brief introduction.</summary>We study how class labels interact with GANs training when introduced and how it improves the sample quality of GANs. Based on the analysis, an improved method for leveraging class labels in GANs has been proposed. An interesting relationship among popular variants of GANs that leverage class labels, including the proposed AM-GANs, is revealed.</details>
-->

<!--
- <details><summary>Click to expand a brief introduction.</summary>We study the cause of training instability of GANs from the perspective of the gradient of the optimal discriminative function. Under a generalized formulation of GANs, we show that: (1) GANs with unregularized discriminative function space generally does not guarantee its convergence, suffering from a *gradient uninformativeness issue*; (2) Lipschitz regularization in the discriminative function space can generally resolve this issue and guarantee the convergence of GANs. This leads to a new family of GANs named Lipschitz GANs. All tested instances of this family consistently outperform WGANs in experiments.</details>

[DBLP](https://dblp.org/pid/56/321.html) 
-->
